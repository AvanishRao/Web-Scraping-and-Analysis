{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPVHjAaDRmrwPjT1KBTdiBQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"shW9zbQDiT_p","executionInfo":{"status":"ok","timestamp":1728337852763,"user_tz":-330,"elapsed":97125,"user":{"displayName":"Avanish Rao","userId":"14087294791528281497"}},"outputId":"954964f2-2be5-4fb6-f4bc-584f86e2c47c"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Analysis complete. Results saved to 'Output Data Structure.xlsx'\n"]}],"source":["import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.corpus import stopwords\n","import re\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","with open('positive-words.txt', 'r',encoding='utf-8',errors='ignore') as file:\n","    positive_words = set(file.read().split())\n","\n","with open('negative-words.txt', 'r', encoding='utf-8', errors='ignore') as file:\n","    negative_words = set(file.read().split())\n","\n","stop_words = set(stopwords.words('english'))\n","\n","def load_custom_stop_words(file_path):\n","    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n","        return set(word.strip().lower() for word in file)\n","\n","custom_stop_words = set()\n","custom_stop_words.update(load_custom_stop_words('StopWords_DatesandNumbers.txt'))\n","custom_stop_words.update(load_custom_stop_words('StopWords_Generic.txt'))\n","custom_stop_words.update(load_custom_stop_words('StopWords_GenericLong.txt'))\n","custom_stop_words.update(load_custom_stop_words('StopWords_Geographic.txt'))\n","custom_stop_words.update(load_custom_stop_words('StopWords_Names.txt'))\n","custom_stop_words.update(load_custom_stop_words('StopWords_Auditor.txt'))\n","custom_stop_words.update(load_custom_stop_words('StopWords_Auditor.txt'))\n","custom_stop_words.update(load_custom_stop_words('StopWords_Currencies.txt'))\n","all_stop_words = stop_words.union(custom_stop_words)\n","\n","def extract_article_text(url):\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","    title = soup.find('h1').text.strip()\n","    paragraphs = soup.find_all('p')\n","    text = ' '.join([p.text for p in paragraphs])\n","    return title, text\n","\n","def clean_text(text):\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    text = text.lower()\n","    words = word_tokenize(text)\n","    cleaned_words = [word for word in words if word not in all_stop_words]\n","    return ' '.join(cleaned_words)\n","\n","def analyze_sentiment(text):\n","    words = word_tokenize(text)\n","    positive_score = sum(1 for word in words if word in positive_words)\n","    negative_score = sum(1 for word in words if word in negative_words)\n","    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n","    subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)\n","    return positive_score, negative_score, polarity_score, subjectivity_score\n","\n","def analyze_readability(text):\n","    sentences = sent_tokenize(text)\n","    words = word_tokenize(text)\n","    avg_sentence_length = len(words) / len(sentences)\n","    complex_words = [word for word in words if len(word) > 2 and len(set(word)) > 2]\n","    percentage_complex_words = len(complex_words) / len(words)\n","    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n","    return avg_sentence_length, percentage_complex_words, fog_index\n","\n","def count_syllables(word):\n","    vowels = 'aeiou'\n","    count = 0\n","    if word[0] in vowels:\n","        count += 1\n","    for index in range(1, len(word)):\n","        if word[index] in vowels and word[index - 1] not in vowels:\n","            count += 1\n","    if word.endswith('e'):\n","        count -= 1\n","    if word.endswith('le'):\n","        count += 1\n","    if count == 0:\n","        count += 1\n","    return count\n","\n","def analyze_text(text):\n","    clean_text_content = clean_text(text)\n","    words = word_tokenize(clean_text_content)\n","    sentences = sent_tokenize(text)\n","    word_count = len(words)\n","    avg_sentence_length = word_count / len(sentences)\n","    complex_words = [word for word in words if count_syllables(word) > 2]\n","    complex_word_count = len(complex_words)\n","    percentage_complex_words = complex_word_count / word_count\n","    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n","    syllable_count = sum(count_syllables(word) for word in words)\n","    syllable_per_word = syllable_count / word_count\n","    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.IGNORECASE))\n","    avg_word_length = sum(len(word) for word in words) / word_count\n","    positive_score, negative_score, polarity_score, subjectivity_score = analyze_sentiment(clean_text_content)\n","    return {\n","        'POSITIVE SCORE': positive_score,\n","        'NEGATIVE SCORE': negative_score,\n","        'POLARITY SCORE': polarity_score,\n","        'SUBJECTIVITY SCORE': subjectivity_score,\n","        'AVG SENTENCE LENGTH': avg_sentence_length,\n","        'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n","        'FOG INDEX': fog_index,\n","        'AVG NUMBER OF WORDS PER SENTENCE': avg_sentence_length,\n","        'COMPLEX WORD COUNT': complex_word_count,\n","        'WORD COUNT': word_count,\n","        'SYLLABLE PER WORD': syllable_per_word,\n","        'PERSONAL PRONOUNS': personal_pronouns,\n","        'AVG WORD LENGTH': avg_word_length\n","    }\n","input_df = pd.read_excel('Input.xlsx')\n","output_df = input_df.copy()\n","for index, row in input_df.iterrows():\n","    url_id = row['URL_ID']\n","    url = row['URL']\n","\n","    try:\n","        title, text = extract_article_text(url)\n","\n","        with open(f'{url_id}.txt', 'w', encoding='utf-8') as file:\n","            file.write(f'{title}\\n\\n{text}')\n","\n","        analysis_results = analyze_text(text)\n","\n","        for key, value in analysis_results.items():\n","            output_df.at[index, key] = value\n","\n","    except Exception as e:\n","        print(f\"Error processing {url_id}: {str(e)}\")\n","output_df.to_excel('Output Data Structure.xlsx', index=False)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"gpnefpHj71Pj"},"execution_count":null,"outputs":[]}]}